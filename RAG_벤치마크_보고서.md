# RAG 시스템 성능 벤치마크 보고서

생성 날짜: 2025년 12월 11일 11:27

## 목차
1. [실험 개요](#실험-개요)
2. [최고 성능 조합 분석](#최고-성능-조합-분석)
3. [PDF별 상세 결과](#pdf별-상세-결과)
4. [종합 분석 및 권장사항](#종합-분석-및-권장사항)

---

## 실험 개요

### 테스트 환경
- **테스트 PDF 문서**: 5개
- **실험당 반복 횟수**: 3회
- **평가 지표**: 임베딩 시간, 검색 시간, 안정성 (표준편차)

### 테스트한 PDF 문서

| PDF 이름 | 청크 수 | 실험 수 |
|---------|--------|--------|
| 공인중개사 법령집 | N/A | 4 |
| 형법 | N/A | 4 |
| 근로 기준법 | N/A | 4 |
| 소프트웨어산업 관련 | N/A | 4 |
| 청년 법령집 | N/A | 3 |

### 테스트한 임베딩 모델
- **HuggingFace MiniLM**: 경량 모델, 빠른 속도
- **Korean SRoBERTa**: 한국어 특화 RoBERTa 기반
- **Korean SimCSE**: 한국어 문맥 임베딩
- **OpenAI Ada-002**: 고품질 상용 모델
- **Cohere Multilingual**: 다국어 지원 상용 모델

### 테스트한 벡터 데이터베이스
- **ChromaDB**: 로컬 벡터 DB
- **FAISS**: Facebook의 고속 유사도 검색
- **Qdrant**: 메모리 기반 벡터 DB

---

## 최고 성능 조합 분석

### 🏆 가장 빠른 임베딩

**조합**: all-MiniLM-L6-v2 + ChromaDB
**테스트 PDF**: 형법
**임베딩 시간**: 0.675 ± 0.057초
**문서당 시간**: 0.0054초

### ⚡ 가장 빠른 검색

**조합**: jhgan/ko-sroberta-multitask + FAISS
**테스트 PDF**: 소프트웨어산업 관련
**검색 시간**: 0.0000 ± 0.0000초

### 🎯 가장 안정적인 조합

**조합**: jhgan/ko-sroberta-multitask + FAISS
**테스트 PDF**: 소프트웨어산업 관련
**임베딩 안정성**: 0.0363초
**검색 안정성**: 0.0000초

---

## PDF별 상세 결과

### 공인중개사 법령집

| 임베딩 모델 | 벡터 DB | 임베딩 시간 (초) | 검색 시간 (초) |
|------------|---------|-----------------|---------------|
| all-MiniLM-L6-v2 | ChromaDB | 2.837 ± 0.105 | 0.0047 ± 0.0000 |
| jhgan/ko-sroberta-multitask | FAISS | 6.186 ± 0.121 | 0.0001 ± 0.0000 |
| BM-K/KoSimCSE-roberta | Qdrant | 17.989 ± 0.377 | 0.0006 ± 0.0001 |
| text-embedding-ada-002 | ChromaDB | 3.548 ± 0.398 | 0.0064 ± 0.0003 |

### 형법

| 임베딩 모델 | 벡터 DB | 임베딩 시간 (초) | 검색 시간 (초) |
|------------|---------|-----------------|---------------|
| all-MiniLM-L6-v2 | ChromaDB | 0.675 ± 0.057 | 0.0012 ± 0.0000 |
| jhgan/ko-sroberta-multitask | FAISS | 1.378 ± 0.039 | 0.0000 ± 0.0000 |
| BM-K/KoSimCSE-roberta | Qdrant | 3.699 ± 0.057 | 0.0004 ± 0.0000 |
| text-embedding-ada-002 | ChromaDB | 1.611 ± 0.447 | 0.0020 ± 0.0002 |

### 근로 기준법

| 임베딩 모델 | 벡터 DB | 임베딩 시간 (초) | 검색 시간 (초) |
|------------|---------|-----------------|---------------|
| all-MiniLM-L6-v2 | ChromaDB | 3.824 ± 0.356 | 0.0058 ± 0.0000 |
| jhgan/ko-sroberta-multitask | FAISS | 10.338 ± 0.793 | 0.0001 ± 0.0000 |
| BM-K/KoSimCSE-roberta | Qdrant | 31.187 ± 0.632 | 0.0006 ± 0.0000 |
| text-embedding-ada-002 | ChromaDB | 4.247 ± 0.891 | 0.0074 ± 0.0001 |

### 소프트웨어산업 관련

| 임베딩 모델 | 벡터 DB | 임베딩 시간 (초) | 검색 시간 (초) |
|------------|---------|-----------------|---------------|
| all-MiniLM-L6-v2 | ChromaDB | 0.910 ± 0.062 | 0.0018 ± 0.0001 |
| jhgan/ko-sroberta-multitask | FAISS | 1.977 ± 0.036 | 0.0000 ± 0.0000 |
| BM-K/KoSimCSE-roberta | Qdrant | 5.384 ± 0.050 | 0.0005 ± 0.0000 |
| text-embedding-ada-002 | ChromaDB | 1.979 ± 0.569 | 0.0030 ± 0.0001 |

### 청년 법령집

| 임베딩 모델 | 벡터 DB | 임베딩 시간 (초) | 검색 시간 (초) |
|------------|---------|-----------------|---------------|
| all-MiniLM-L6-v2 | ChromaDB | 8.289 ± 0.071 | 0.0003 ± 0.0000 |
| jhgan/ko-sroberta-multitask | FAISS | 21.763 ± 1.119 | 0.0001 ± 0.0000 |
| BM-K/KoSimCSE-roberta | Qdrant | 66.738 ± 3.369 | 0.0010 ± 0.0000 |

---

## 종합 분석 및 권장사항

### 성능 분석 요약

#### 1. 임베딩 모델 비교

**속도 순위**:
1. HuggingFace MiniLM (가장 빠름)
2. OpenAI Ada-002
3. Korean SRoBERTa
4. Korean SimCSE (가장 느림)

**특징**:
- **MiniLM**: 가장 빠르지만 경량 모델로 품질이 다소 낮을 수 있음
- **Korean 모델들**: 한국어에 특화되어 있어 한국어 문서에 더 좋은 결과 기대
- **OpenAI Ada-002**: 균형잡힌 성능, API 비용 발생

#### 2. 벡터 데이터베이스 비교

**검색 속도 순위**:
1. FAISS (가장 빠름, 마이크로초 단위)
2. Qdrant
3. ChromaDB

**특징**:
- **FAISS**: 검색 속도가 압도적으로 빠름, 메모리 효율적
- **ChromaDB**: 사용하기 쉬움, 로컬 개발에 적합
- **Qdrant**: 메모리 기반, 빠른 검색, 스케일링 가능

### 사용 사례별 권장사항

#### 📌 대용량 문서, 빠른 처리 필요
- **권장**: HuggingFace MiniLM + FAISS
- **이유**: 가장 빠른 임베딩과 검색 속도
- **Trade-off**: 임베딩 품질이 다소 낮을 수 있음

#### 📌 한국어 문서, 높은 품질 필요
- **권장**: Korean SRoBERTa + FAISS
- **이유**: 한국어 특화 모델, 빠른 검색
- **Trade-off**: 임베딩 시간이 MiniLM보다 2-3배 느림

#### 📌 최고 품질, 비용 무관
- **권장**: OpenAI Ada-002 + ChromaDB
- **이유**: 고품질 임베딩, 안정적인 성능
- **Trade-off**: API 비용 발생

#### 📌 로컬 환경, 비용 제로
- **권장**: HuggingFace MiniLM + ChromaDB
- **이유**: 완전 무료, 로컬 실행, 쉬운 설정
- **Trade-off**: 품질과 속도 trade-off

### 성능 최적화 팁

1. **청크 크기 조정**: 500자가 기본이지만, 문서 특성에 따라 조정 필요
2. **배치 처리**: 대량 문서는 배치로 처리하여 효율성 향상
3. **인덱스 타입**: FAISS의 경우 IVF 인덱스 사용 시 더 빠른 검색 가능
4. **캐싱**: 자주 사용하는 임베딩은 캐싱하여 재사용

---

## 결론

이 벤치마크 결과는 다음을 보여줍니다:

1. **속도와 품질의 Trade-off**: 경량 모델은 빠르지만 품질이 낮고, 큰 모델은 느리지만 품질이 높음
2. **벡터 DB의 중요성**: FAISS가 검색 속도에서 압도적으로 우수
3. **한국어 특화 모델의 필요성**: 한국어 문서에서는 한국어 특화 모델이 더 나은 결과를 제공할 가능성
4. **실용적 선택**: 대부분의 경우 HuggingFace MiniLM + FAISS 조합이 가장 실용적

---

*이 보고서는 자동으로 생성되었습니다.*
